== Tuesday ==

Multiple Testing Corrections

Population alpha (my_alpha) -> Sample A \
					Model (e.g. my_alpha - my_beta) _> Inference
Population beta (my_beta) -> Sample B  /

H_0: my_alpha-my_beta = 0
H_1: my_alpha-my_beta != 0

p-value: Pr(data|H_0)
p-value: Pr(|delta my>=||my_alpha-my_beta|| |H_0)

Randomized trial for a drug company: typical significance level = 0.05
If you measure 20 variables you are expected to find at least one significant with the same level

False discovery rate ("FDR")

5% of the measurements at sig.lvl = 0.05 will actually be adherent to the H_0 by chance
but we do not know which

We control for false discovery rate

P-values are uniformely distributed, p.val€R[0,1]

Normal high-throughput experiment (mixture of H_0 and H-1)

Numbers of findings below t: (total numbers of features considered significant)
S(t) = |{p_i<=t}|
Number of null p-values below t:
F(t) = Pi_0*m*t

where M=|{p_i}|  # the number of p-values

FDR(t) = EV{F(t)/{S(t)} = Pi_0*M*t/(|{P_i<=t}|)

q(P_i) = min_(t>=P_i)FDR(t)

Pi_0(lamba) = |{p_i}>lambda}|/M(1-lambda)


== Afternoon ==

Complexity theory - complexity classes

P: the problems for which E an algorithm with polynomial time complexity
O(n), O(n^2), O(n^12)
"Practical"
"The class of problems that we can handle today"

NP: the problems for which a solution can be verified in polynomial time
	"Non-deterministic polynomial time"
	Classic: satisfiability of Boolean formulas
	=> O(2^n) best estimate
	
	P is a subset of NP

	NP complete:= belongs to the set of the hardest problems in NP

Graphs
 * Hamiltonian path
	Input: Graph G=(V,E)
	Output: True iff exists a path visiting all vertices exactly once

	A decision problem since the answere is either true or false

	NP complete! Seems O(2^n)

	You have DNA-reads, and their overlaps
	Form a vertex for each read, and an edge (x,y) if significant overlap of reads x&y
	Is there a way to merge these reads into a contig?
	Ideally, involve every read exactly once

	Improvement: weight edges with overlap

	Travelling salesman

*Euler path
	Input: Graph G=(V,E)
	Output: True iff exists a path path visiting each edge exactly once

	Theorem: An Euler path exists iff all vertices except two have even-numbered degrees
	
	[(1,2), (2,3), (3,1] a triangle etc
	O(|V|+|E|)

*de Bruijn graph
	For each read and its (k+1)-mers (substrins of length k+1), create (if needed)
	two vertices, for the k-mer prefix and the k-mer suffix, and an
	edge between them

	k=4
	ACGTAATC

	ACGT -> CGTA -> 
			GTAA 
	AATC <- TAAT <-
	
	Possible to solve in linear time


Problem: databases are biased, many similar species and data
Desire: train ML software to recognize structural features

Name: Max. independent set problem } NP complete
Complement: Max clique problem

Suggestion: weight input features by how well a group of species are represented

Old solution: if two proteins are more than x% idential, we only want to keep one

Proposition: create a vertex for each protein
	     create an edge between vertices from highly similar proteins


Impleentation
	*Edge list
	*Adjacency list - Python dict over vertices,
			  storing a list of adjacent verties
	*Adjacency matrix - A_(v,w) = 1 iff (v,w)€E
				      0 otherwise
			    CAVE: Bad for large sparse graphs
			    |E| = a(|V|^2) => "sparse graph"

First: look at modules rather than implementing them yourself! "networkx" is a good
already existing module

Graph traversal
¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨

	A building block for many algorithms
	*Good for iterating over vertices in "smart" order
	*It helps you with questions like reachability
	*Building blocks for other algorithms

	DFS - depth first search: "Attack! Fast forward!"
	BFS - breadth first search: "Careful! Look at neighbours first!"


Spruce genome project
	Large genome ~20Gbp

Most DNA is repeats and duplicates	}
~30,000 genes				} => hard assembly problem
Diploid, highly heterozygous		}

More data
	version 1 had ~30x coverage (each gene had 30 reads)
	some fosmid sequences (40knt sequence), pooled, 1000 fosmids/pool

Average contig length ~= 1kbp
Hoping for 1000 contigs of length 40k

Reality: average contig length: ~10k, each fosmid broken up in 4 pieces

Adding 2000 pools, ~3x coverage

version 2 input:
	5 WGS assemblies (contig ~1k on avg)
	Long fosmid pool contigs (that are roughly ~10k long)

No long-read assembler works on this data
Have collaborators that have computed overlaps data for all contigs

	Using connected components we get contigs
	Avoided taking away high-degree-nodes but not venture out along their
	edges

================================
